
# DataOps CI/CD Pipeline: Automated ETL with GitHub Actions

[![CI/CD Pipeline](https://github.com/JosephNjiru/dataops-cicd-pipeline-github-actions/actions/workflows/ci-cd.yml/badge.svg)](https://github.com/JosephNjiru/dataops-cicd-pipeline-github-actions/actions/workflows/ci-cd.yml)
[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=flat&logo=docker&logoColor=white)](https://docker.com)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **Project Status**: âœ… Fully functional. All tests pass in CI/CD and local environments.

This project demonstrates the application of DevOps principles to data engineering through a complete Continuous Integration and Continuous Deployment (CI/CD) pipeline for a serverless ETL job. Leveraging GitHub Actions, it automates testing, validation, and packaging of data transformation code, ensuring robust and reliable data pipelines.

## ğŸ¯ Business Value & Impact

Traditional data engineering workflows often lack automated quality gates, leading to operational risks where untested code changes can disrupt critical data pipelines. This project bridges that gap by implementing **DataOps** practicesâ€”applying DevOps methodologies to data engineering.

### Key Achievements
- âœ… **Zero-downtime deployments** with automated testing gates
- âœ… **Data quality assurance** through schema validation
- âœ… **Reproducible environments** via containerization
- âœ… **Rapid iteration** with automated CI/CD workflows
- âœ… **Production-ready artifacts** delivered via Docker images

## ğŸ—ï¸ Architecture Overview

```mermaid
graph LR
    A[ğŸ‘¨â€ğŸ’» Developer Push] --> B[GitHub Actions Trigger]
    B --> C[ğŸ§ª Test Suite<br/>Unit + Integration + Data Quality]
    C --> D[ğŸ“¦ Docker Build<br/>Multi-stage Image]
    D --> E[ğŸ³ Push to Registry<br/>Tagged Release]

    style A fill:#e1f5fe
    style C fill:#c8e6c9
    style D fill:#fff3e0
    style E fill:#f3e5f5
```

**Pipeline Flow:**
1. **Code Push** â†’ Triggers automated workflow
2. **Quality Gates** â†’ Comprehensive test suite execution
3. **Artifact Creation** â†’ Docker image build and tagging
4. **Deployment Ready** â†’ Immutable container available for deployment

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| ğŸƒ **CI/CD** | GitHub Actions | Workflow orchestration and automation |
| ğŸ **Runtime** | Python 3.11+ | ETL application development |
| ğŸ§ª **Testing** | pytest | Unit and integration test framework |
| ğŸ“Š **Data Quality** | Pandera | Schema validation and data quality checks |
| ğŸ“¦ **Containerization** | Docker | Environment reproducibility |
| ğŸ—„ï¸ **Integration Testing** | SQLite | Database testing and validation |
| ğŸ“ˆ **Visualization** | Matplotlib | Pipeline metrics and reporting |

## ğŸ“ Project Structure

```
dataops-cicd-pipeline-github-actions/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci-cd.yml              # ğŸš€ CI/CD pipeline configuration
â”œâ”€â”€ src/
â”‚   â””â”€â”€ etl_pipeline/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ handler.py             # ğŸ Core ETL logic
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_handler.py            # âœ… Unit tests
â”‚   â”œâ”€â”€ test_db_integration.py     # ğŸ—„ï¸ Database integration tests
â”‚   â””â”€â”€ test_data_quality.py       # ğŸ“Š Data quality validation
â”œâ”€â”€ Dockerfile                     # ğŸ“¦ Container build instructions
â”œâ”€â”€ requirements.txt               # ï¿½ Python dependencies
â”œâ”€â”€ generate_paper_charts.py       # ğŸ“Š Visualization scripts
â””â”€â”€ README.md                      # ğŸ“– Documentation
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11 or higher
- Git
- Docker (for containerized execution)

### Local Development Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/JosephNjiru/dataops-cicd-pipeline-github-actions.git
   cd dataops-cicd-pipeline-github-actions
   ```

2. **Set up virtual environment**
   ```bash
   # Windows
   python -m venv .venv
   .\.venv\Scripts\activate

   # macOS/Linux
   python3 -m venv .venv
   source .venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run test suite**
   ```bash
   pytest
   ```

### Docker Execution

Build and run the ETL pipeline in a container:

```bash
# Build the image
docker build -t dataops-etl .

# Run the container
docker run --rm dataops-etl
```

## ğŸ“Š Data Quality & Validation

The pipeline implements comprehensive data quality checks using Pandera:

- **Schema Validation**: Enforces data types and constraints
- **Business Rules**: Validates calculated fields (total_price, total_sales)
- **Data Integrity**: Ensures referential integrity and value ranges

```python
# Example: Pandera schema validation
class TransformedSalesSchema(pa.SchemaModel):
    order_id: Series[int] = pa.Field(nullable=False)
    total_price: Series[float] = pa.Field(ge=0, nullable=False)
    # ... additional validations
```

## ğŸ“ˆ Performance Analytics

Generate publication-quality performance visualizations:

```bash
python generate_paper_charts.py
```

**Generated Charts:**
- Pipeline stage duration comparisons
- Data quality validation results
- Statistical distribution of execution times

## ğŸ¤ Contributing

We welcome contributions! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Guidelines
- Ensure all tests pass before submitting PRs
- Follow PEP 8 style guidelines
- Add tests for new functionality
- Update documentation as needed

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Built with â¤ï¸ for the data engineering community
- Inspired by real-world DataOps challenges
- Special thanks to the open-source ecosystem

---

**Ready to revolutionize your data pipelines?** â­ Star this repo and join the DataOps movement!
